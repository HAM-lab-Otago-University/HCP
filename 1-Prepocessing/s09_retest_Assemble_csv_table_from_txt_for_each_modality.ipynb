{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/DataD800/Alina/retest_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes for tables from different atlases (Glasser atlas, Destrieux atlas (freesurfer), subcortex names)\n",
    "index_gla = np.loadtxt('/media/DataD800/Alina/atlases/gla_index.txt', dtype=str)\n",
    "index_subc = np.loadtxt('/media/DataD800/Alina/atlases/fs_index_subc.txt', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assemble txt file to csv table for tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = sorted([elem for elem in os.listdir(path)  if 'analysis_s2' in elem ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tables with Glasser parcellation and subcortical parcellation and save it to task folder\n",
    "for task in tasks:\n",
    "    #read subj names, they may differ from folder to folder, so read each time\n",
    "    subjects = [nm[0:6] for nm in sorted(os.listdir(path+str(task)+'/analysis/cort_MSMAll_av_Glasser_txt'))]\n",
    "    #Glasser atlas\n",
    "    dct_gla={}\n",
    "    for subject in subjects:\n",
    "        dct_gla[subject] = np.loadtxt(path+str(task)+'/analysis/cort_MSMAll_av_Glasser_txt/'+str(subject)+'.MSMAll.aver_parc.txt', dtype=float)\n",
    "    df_gla= pd.DataFrame(dct_gla, index_gla).T\n",
    "    df_gla.to_csv(path+str(task)+'/'+str(task.split('_')[2])+'_table_Glasser.csv')    \n",
    "    #FS subcortex\n",
    "    dct_subc={}\n",
    "    subjects = [nm[0:6] for nm in sorted(os.listdir(path+str(task)+'/analysis/subc_FS_MSMAll_txts'))]\n",
    "    for subject in subjects:\n",
    "        vec=[]\n",
    "        for file in index_subc:\n",
    "            vec+= [np.loadtxt(path+str(task)+'/analysis/subc_FS_MSMAll_txts/'+str(subject)+'/'+str(subject)+'.FS_subcort.'+str(file)+'.mean.txt')]\n",
    "        dct_subc[subject]=pd.Series(np.array(vec))\n",
    "    df_subc = pd.DataFrame(dct_subc)\n",
    "    df_subc.index= index_subc\n",
    "    df_subc = df_subc.T\n",
    "    df_subc.to_csv(path+str(task)+'/'+str(task.split('_')[2])+'_table_FSsubcort.csv')\n",
    "    df_tabl = pd.concat([df_gla,df_subc], axis=1)\n",
    "    df_tabl.to_csv(path+str(task)+'/'+str(task.split('_')[2])+'_table_Glasser_FSsubcort.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assemble txt file to csv table for rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rest = path+'3T_rfMRI_REST_fix/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = sorted(set([nm.split('.')[0] for nm in os.listdir(path_rest+'analysis')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan in scans:\n",
    "    os.mkdir(path_rest+'analysis/'+str(scan)+'.ind_tables_timecourse')\n",
    "    path_out = path_rest+'analysis/'+str(scan)+'.ind_tables_timecourse'\n",
    "    subjects = [nm[0:6] for nm in sorted(os.listdir(path_rest+'analysis/'+str(scan)+'.cort_MSMAll_av_Glasser_txt'))]\n",
    "    for subject in subjects:\n",
    "\n",
    "        df_gla = pd.read_csv(path_rest+'analysis/'+str(scan)+'.cort_MSMAll_av_Glasser_txt/'+str(subject)+'.MSMAll.aver_parc.txt',\n",
    "                   sep='\\t', header=None)\n",
    "        df_gla.index = index_gla\n",
    "        df_gla.to_csv(path_out+'/'+str(subject)+'.Glasser_timecouse.csv')\n",
    "\n",
    "    subjects = [nm[0:6] for nm in sorted(os.listdir(path_rest+'analysis/'+str(scan)+'.subc_FS_MSMAll_txts'))]\n",
    "    for subject in subjects:\n",
    "        dct_ssubc={}\n",
    "        for file in index_subc:\n",
    "            dct_ssubc[file]= np.loadtxt(path_rest+'analysis/'+str(scan)+'.subc_FS_MSMAll_txts/'+str(subject)+'/'+str(subject)+'.FS_subcort.'+str(file)+'.mean.txt', dtype=float)\n",
    "        df_ssubc = pd.DataFrame(dct_ssubc).T\n",
    "        df_ssubc.to_csv(path_out+'/'+str(subject)+'.FSsubcort_timecouse.csv')\n",
    "\n",
    "        df_1 = pd.read_csv(path_out+'/'+str(subject)+'.Glasser_timecouse.csv', index_col=0)\n",
    "        df_2 = pd.read_csv(path_out+'/'+str(subject)+'.FSsubcort_timecouse.csv', index_col=0)\n",
    "        df_tabl = pd.concat([df_1, df_2], axis=0)\n",
    "        #display(df_tabl)\n",
    "        df_tabl.to_csv(path_out+'/'+str(subject)+'.Glasser_FSsubcort_timecouse.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(path_rest+'analysis/'+str(scans[0])+'.ALL_scans_timecourses')\n",
    "path_out = path_rest+'analysis/'+str(scans[0])+'.ALL_scans_timecourses'\n",
    "os.mkdir(path_out+'/cormatrix')\n",
    "os.mkdir(path_out+'/fulltimecourse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "#Check that all subjects have all 4 files\n",
    "\n",
    "#all subj existed in all 4 folders\n",
    "dct_list={}\n",
    "for scan in scans:\n",
    "    dct_list[scan] = sorted(set([l.split('/')[-1][:6] for l in glob.glob(path_rest+'analysis/'+str(scan)+'.ind_tables_timecourse/*.Glasser_FSsubcort_timecouse.csv')]))\n",
    "\n",
    "sj_nm = list(set(dct_list[scans[0]]) & set(dct_list[scans[1]]) & set(dct_list[scans[2]]) & set(dct_list[scans[3]]))\n",
    "\n",
    "print(len(sj_nm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143325 is too short 4539 TR out of 4800\n"
     ]
    }
   ],
   "source": [
    "#concat all 4 tables into one for  each subj\n",
    "dct_tmcors = {}\n",
    "for subject in sj_nm:\n",
    "    df_t = pd.DataFrame()\n",
    "    for scan in scans:\n",
    "        df = pd.read_csv(path_rest+'analysis/'+str(scan)+'.ind_tables_timecourse/'+str(subject)+'.Glasser_FSsubcort_timecouse.csv', index_col=0)\n",
    "        df_t = pd.concat([df_t, df], axis=1, ignore_index=True)\n",
    "    dct_tmcors[subject] = df_t\n",
    "\n",
    "# check if somebody has shorter scan lenghts\n",
    "keys = sorted(dct_tmcors.keys())   \n",
    "for key in keys:\n",
    "    if len(dct_tmcors[key].columns) < (1200*4):\n",
    "        print(key, 'is too short', len(dct_tmcors[key].columns), 'TR out of', 1200*4)\n",
    "        \n",
    "\n",
    "#Save\n",
    "for key in dct_tmcors.keys():\n",
    "    dct_tmcors[key].to_csv(path_out+'/fulltimecourse/'+str(key)+'.full_rest_Glasser_FSsubcort_timecourse.csv')\n",
    "    dct_tmcors[key].T.corr().to_csv(path_out+'/cormatrix/'+str(key)+'.full_rest_Glasser_FSsubcort_cormatrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build group table of functional connectivity\n",
    "subjects = [s[:6] for s in  sorted(os.listdir(path_out+'/cormatrix'))]\n",
    "dct_fc = {}\n",
    "for subject in subjects:\n",
    "    df = pd.read_csv(path_out+'/cormatrix/'+str(subject)+'.full_rest_Glasser_FSsubcort_cormatrix.csv', index_col=0)\n",
    "    vec = []\n",
    "    nms = []\n",
    "    for i in range(len(df.index)):\n",
    "        j=i+1\n",
    "        while j<len(df.index):\n",
    "            vec+=[df.iloc[i,j]]\n",
    "            nms+=[(str(df.index[i])+'-'+str(df.index[j]))]\n",
    "            j+=1\n",
    "    dct_fc[subject] = pd.Series(np.array(vec, dtype=float), index=nms)\n",
    "df_fc = pd.DataFrame(dct_fc)\n",
    "df_fc.T.to_csv(path_rest+'group_rest_FC.csv') #save\n",
    "\n",
    "#r-to-z transformation\n",
    "df_fc_z = np.arctanh(df_fc)\n",
    "df_fc_z.T.to_csv(path_rest+'group_rest_FC_z.csv') #save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assemble anatomy tables (cort, subc, area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_anat = path+'3T_Structural_preproc_extended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cortical thickness\n",
    "#load files\n",
    "df_l = pd.read_csv(path_anat+'/aparc_stats_thickness_a2009s_lh.txt', index_col=0)\n",
    "df_r = pd.read_csv(path_anat+'/aparc_stats_thickness_a2009s_rh.txt', index_col=0)\n",
    "#shrink col names\n",
    "left_col = [col.replace('_thickness', '') for col in df_l.columns]\n",
    "right_col = [col.replace('_thickness', '') for col in df_r.columns]\n",
    "df_l.columns = left_col\n",
    "df_r.columns = right_col\n",
    "#del name of index\n",
    "df_l.index.name = None\n",
    "df_r.index.name = None\n",
    "\n",
    "#unite table into one excluding mean thickness column (last one)\n",
    "df_thck = pd.concat([df_l.iloc[:,:-1], df_r.iloc[:,:-1]], axis=1)\n",
    "df_thck.to_csv(path_anat+'/FS_anatomy_cortical_thickness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cortical area\n",
    "#load files\n",
    "df_al = pd.read_csv(path_anat+'/aparc_stats_area_a2009s_lh.txt', index_col=0)\n",
    "df_ar = pd.read_csv(path_anat+'/aparc_stats_area_a2009s_rh.txt', index_col=0)\n",
    "#shrink col names\n",
    "left_col = [col.replace('_area', '') for col in df_al.columns]\n",
    "right_col = [col.replace('_area', '') for col in df_ar.columns]\n",
    "df_al.columns = left_col\n",
    "df_ar.columns = right_col\n",
    "#del name of index\n",
    "df_al.index.name = None\n",
    "df_ar.index.name = None\n",
    "\n",
    "#unite table into one excluding WhiteSurfArea column (last one)\n",
    "df_area = pd.concat([df_al.iloc[:,:-1], df_ar.iloc[:,:-1]], axis=1)\n",
    "df_area.to_csv(path_anat+'/FS_anatomy_cortical_area.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subcortex volume\n",
    "df_sc = pd.read_csv(path_anat+'/aseg_stats.txt', index_col=0)\n",
    "df_sc.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "subc_19label = ['Left-Cerebellum-Cortex',\n",
    " 'Left-Thalamus-Proper',\n",
    " 'Left-Caudate',\n",
    " 'Left-Putamen',\n",
    " 'Left-Pallidum',\n",
    " 'Brain-Stem',\n",
    " 'Left-Hippocampus',\n",
    " 'Left-Amygdala',\n",
    " 'Left-Accumbens-area',\n",
    " 'Left-VentralDC',\n",
    " 'Right-Cerebellum-Cortex',\n",
    " 'Right-Thalamus-Proper',\n",
    " 'Right-Caudate',\n",
    " 'Right-Putamen',\n",
    " 'Right-Pallidum',\n",
    " 'Right-Hippocampus',\n",
    " 'Right-Amygdala',\n",
    " 'Right-Accumbens-area',\n",
    " 'Right-VentralDC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter columns\n",
    "df_sc = df_sc.loc[:, sorted(subc_19label)]\n",
    "df_sc.to_csv(path_anat+'/FS_anatomy_subcortical_volume.csv') #save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assembling in-scanner movements into tables for task and rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMOTION', 'GAMBLING', 'LANGUAGE', 'MOTOR', 'RELATIONAL', 'SOCIAL', 'WM']\n"
     ]
    }
   ],
   "source": [
    "folder = [l.split('/')[-1] for l in glob.glob(path+'*tfMRI*_preproc')]\n",
    "f_names = sorted([n.split('_')[-2] for n in folder])\n",
    "print(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_mov={}\n",
    "for mod in f_names:\n",
    "    path_mv = path+'3T_tfMRI_'+str(mod)+'_preproc'\n",
    "    path_in = path_mv+'/analysis/confounds'\n",
    "    subjects = sorted(os.listdir(path_in))\n",
    "    vec = []\n",
    "    for subject in subjects:\n",
    "        if os.path.isfile(path_in+'/'+str(subject)+'/LR/Movement_RelativeRMS_mean.txt') and os.path.isfile(path_in+'/'+str(subject)+'/RL/Movement_RelativeRMS_mean.txt') == True:\n",
    "            m1 = np.loadtxt(path_in+'/'+str(subject)+'/LR/Movement_RelativeRMS_mean.txt')\n",
    "            m2 = np.loadtxt(path_in+'/'+str(subject)+'/RL/Movement_RelativeRMS_mean.txt')\n",
    "            m= np.mean([m1,m2])\n",
    "            vec+=[m]\n",
    "            sub+=[subject]\n",
    "    dct_mov[mod] = pd.Series(vec, index=sub)\n",
    "    dct_mov[mod].to_csv(path_mv+'/'+str(mod)+'_all_subj_movements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mov = pd.DataFrame(dct_mov)\n",
    "df_mov.columns = [col.lower()[:3] for col in df_mov.columns]\n",
    "#display(df_mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_rmov={}\n",
    "for rest in ['REST1', 'REST2']:\n",
    "    subjects = sorted(os.listdir(path+'3T_rfMRI_'+str(rest)+'_preproc/analysis/confounds/'))\n",
    "    vec = []\n",
    "    sub=[]\n",
    "    for subject in subjects:\n",
    "        if os.path.isfile(path+'3T_rfMRI_'+str(rest)+'_preproc/analysis/confounds/'+str(subject)+'/LR/Movement_RelativeRMS_mean.txt') and os.path.isfile(path+'3T_rfMRI_'+str(rest)+'_preproc/z_analysis/confounds/'+str(subject)+'/RL/Movement_RelativeRMS_mean.txt') ==True:\n",
    "            m1 = np.loadtxt(path+'3T_rfMRI_'+str(rest)+'_preproc/analysis/confounds/'+str(subject)+'/LR/Movement_RelativeRMS_mean.txt')\n",
    "            m2 = np.loadtxt(path+'3T_rfMRI_'+str(rest)+'_preproc/analysis/confounds/'+str(subject)+'/RL/Movement_RelativeRMS_mean.txt')\n",
    "            m= np.mean([m1,m2])\n",
    "            vec+=[m]\n",
    "            sub+=[subject]\n",
    "    dct_rmov[rest] = pd.Series(vec, index=sub)\n",
    "    dct_rmov[rest].to_csv(path+'3T_rfMRI_'+str(rest)+'_preproc/'+str(rest)+'_all_subj_movements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mov['rest'] = pd.DataFrame(dct_rmov).dropna(axis=0).T.mean().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mov.to_csv(path+'3T_tfMRI_WM_preproc/all_mods_all_subj_mov.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. assembling total brain volume table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for main set only\n",
    "beh_tab_1200 = pd.read_csv(path+'Behavioral_tables/unrestricted.csv', index_col=0)\n",
    "col_volume = ['FS_IntraCranial_Vol', 'FS_TotCort_GM_Vol', 'FS_SubCort_GM_Vol', 'FS_Tot_WM_Vol', 'FS_BrainSegVol_eTIV_Ratio']\n",
    "beh_tab_1200.loc[:,col_volume].to_csv(path+'3T_Structural_preproc_extended/totbrainvol_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for retest set only!!\n",
    "retest_fs_tab = pd.read_csv(path+'Behavioral_tables/unrestricted_hcp_freesurfer.csv', index_col=0)\n",
    "col_volume = ['FS_IntraCranial_Vol', 'FS_TotCort_GM_Vol', 'FS_SubCort_GM_Vol', 'FS_Tot_WM_Vol', 'FS_BrainSegVol_eTIV_Ratio']\n",
    "retest_fs_tab.loc[:,col_volume].to_csv(path+'3T_Structural_preproc_extended/totbrainvol_table.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
